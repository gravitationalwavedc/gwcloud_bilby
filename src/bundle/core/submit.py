import json
import os
import re
import subprocess
from tempfile import NamedTemporaryFile

from bilby_pipe.job_creation.dag import Dag
from bilby_pipe.job_creation.slurm import SubmitSLURM
from bilby_pipe.main import MainInput, generate_dag
from bilby_pipe.parser import create_parser
from bilby_pipe.utils import parse_args

from core.misc import working_directory, get_scheduler
from db import get_unique_job_id, update_job
from settings import scheduler_env, scheduler


def bilby_ini_to_args(ini):
    """
    Parses an ini string in to an argument Namespace

    :params ini: The ini string to parse
    :return: An ArgParser Namespace of the parsed arguments from the ini
    """

    # Create a bilby argument parser
    parser = create_parser()

    # Bilby pipe requires a real file in order to parse the ini file
    with NamedTemporaryFile() as f:
        # Write the temporary ini file
        f.write(ini.encode('utf-8'))

        # Make sure the data is written to the temporary file
        f.flush()

        # Read the data from the ini file
        args, unknown_args = parse_args([f.name], parser)

    return args


def prepare_ini_data(params, wk_dir):
    """
    Takes the ini content generated by the UI and sanitizes and updates the ini file for execution as a job

    :param params: The original json object string sent from the UI
    :param wk_dir: The working directory of the job
    :return: The updated args ready to be executed as a job
    """

    # Parse the json object from the ui
    params = json.loads(params)

    # Get the ini content
    ini = params['ini_string']

    # First parse the ini file to args
    args = bilby_ini_to_args(ini)

    # Next, override required arguments

    ################################################################################
    # Job submission arguments
    # How the jobs should be formatted, e.g., which job scheduler to use.
    ################################################################################

    # Output directory
    args.outdir = wk_dir

    # Ignore transfer files because we're not using condor. This resolves the problem with relpath of the CWD path
    # being ".", which bilby_pipe refuses to use as a valid output directory
    args.transfer_files = False

    # Time after which the job will be self-evicted. After this, condor will restart the job. Default is 28800.
    # This is used to decrease the chance of HTCondor hard evictions
    args.periodic_restart_time = 2147483647

    # Format submission script for specified scheduler. Currently implemented: SLURM
    args.scheduler = scheduler

    # Environment scheduler sources during runtime
    args.scheduler_env = scheduler_env

    ################################################################################
    # Output arguments
    # What kind of output/summary to generate.
    ################################################################################

    # Create diagnostic and posterior plots
    args.create_plots = True

    # Create calibration posterior plot
    args.plot_calibration = False

    # Create intrinsic and extrinsic posterior corner plots
    args.plot_corner = True

    # Create 1-d marginal posterior plots
    args.plot_marginal = True

    # Create posterior skymap
    args.plot_skymap = True

    # Create waveform posterior plot
    args.plot_waveform = True

    # Format for making bilby_pipe plots, can be [png, pdf, html]. If specified format is not supported, will
    # default to png.
    args.plot_format = "png"

    # Create a PESummary page
    args.create_summary = False

    ################################################################################
    # Waveform arguments
    # Setting for the waveform generator
    ################################################################################

    # Turns on waveform error catching
    args.catch_waveform_errors = True

    return args


def run_data_generation(data_gen_command, wk_dir):
    """
    Uses the original data generation step command to run the data generation step locally. This is done for jobs which
    require GWOSC or real data, since the data generation step can not be executed on a compute node.

    :param data_gen_command: The original data generation command
    :param wk_dir: The working directory of the job
    :return: Nothing
    """
    # Get the error and output log paths
    error_file = None
    output_file = None
    for bit in data_gen_command.split(' '):
        if '--error=' in bit:
            error_file = bit.split('--error=')[-1]

        if '--output' in bit:
            output_file = bit.split('--output=')[-1]

    # Get the last parameter to sbatch, which is the script to run to generate the data
    data_gen_command = data_gen_command.split(' ')[-1]

    # Remove the closing brackets
    data_gen_command = data_gen_command.replace(')', '')

    # Strip any newlines or whitespace
    data_gen_command = data_gen_command.strip()

    # Run the data generation
    os.sync()
    with subprocess.Popen(
        f"/bin/bash {os.path.abspath(os.path.join(wk_dir, data_gen_command))}",
        cwd=wk_dir,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        shell=True
    ) as p:
        p.wait()

        # Get the output from the data generation command
        stdout, stderr = p.communicate()

        # Write the data generation output to output files
        with open(output_file, "w") as f:
            f.write(stdout.decode())
        with open(error_file, "w") as f:
            f.write(stderr.decode())


def refactor_slurm_data_generation_step(slurm_script):
    """
    When using real or GWOSC data, the data generation step needs to be removed from the slurm jobs and executed
    locally. This function refactors the slurm master submission script to remove the data generation job step, and
    returns that command so it can be executed by this script.

    :param slurm_script: The path to the master slurm job submission script
    :return: The data generation command
    """
    # Read the lines from the submit script
    with open(slurm_script, 'r') as f:
        slines = f.readlines()

    # Find the line for data generation and the first echo after that, then remove the dependency from the following
    # sbatch command
    data_gen_idx = None
    data_gen_command = None
    new_lines = []
    generation_jid = None
    echo_found = False
    for line in slines:
        # Check for the sbatch command to generate the data
        if 'log_data_generation' in line:
            data_gen_idx = line
            data_gen_command = line
            generation_jid = data_gen_command.split('=')[0]

            # Nothing more to do, exclude this line from the new sbatch script
            continue

        # Check for the first echo command after the sbatch command
        if data_gen_idx and 'echo' in line and not echo_found:
            echo_found = True
            # Nothing more to do, exclude this line from the new sbatch script
            continue

        # Check if this line is the next sbatch command using jid0 as a
        if data_gen_idx and '--dependency=afterok:${' + generation_jid + '[-1]}' in line:
            # Remove the dependenc
            line = line.replace('--dependency=afterok:${' + generation_jid + '[-1]}', '')

        new_lines.append(line)

    # Replace any triple newlines with double newlines
    script_content = re.sub('\n\n+', '\n\n', ''.join(new_lines))

    # Write the updated lines to the job submission script
    with open(slurm_script, 'w') as f:
        f.write(script_content)

    return data_gen_command


def write_submission_scripts(inputs):
    """
    Writes the slurm submission scripts using the MainInput inputs, and returns the slurm master script path

    :param inputs: The MainInput object with the complete input information for the job
    :return: The path to the slurm master script
    """
    # Generate the submission scripts
    generate_dag(inputs)
    # Get the name of the slurm script
    dag = Dag(inputs)
    _slurm = SubmitSLURM(dag)
    slurm_script = _slurm.slurm_master_bash
    return slurm_script


def write_ini_file(args):
    """
    Takes the parser args and writes the complete ini file in the job output directory

    :param args: Args as generated by the bilby_pipe parser
    :return: The updated Args, and the MainInput object representing the complete bilby_pipe input object
    """
    # Create an argument parser
    parser = create_parser()

    # Because we don't know the correct ini file name yet, we need to save the ini data to a temporary file
    # and then read the data back in to create a MainInput object, which we can then use to get the name of the ini
    # file
    with NamedTemporaryFile() as f:
        # Write the temporary ini file
        parser.write_to_file(f.name, args, overwrite=True)

        # Make sure the data is flushed
        f.flush()

        # Read the data from the ini file
        args, unknown_args = parse_args([f.name], parser)

        # Generate the Input object so that we can determine the correct ini file
        inputs = MainInput(args, unknown_args)

    # Write the real ini file
    parser.write_to_file(inputs.complete_ini_file, args, overwrite=True)

    return args, inputs


def create_working_directory(details):
    """
    Creates and enters (chdir) the working directory for the job. ie the output directory
    :param details: The job details provided by the client
    :return: The working (output) directory for the job
    """
    # Get the working directory
    wk_dir = working_directory(details)

    # Create the working directory
    os.makedirs(wk_dir, exist_ok=True)

    # Change to the working directory
    os.chdir(wk_dir)

    return wk_dir


def submit(details, ini_string):
    print("Submitting new job...")

    # Create and enter the working directory
    wk_dir = create_working_directory(details)

    # Get the ini ready for job submission
    args = prepare_ini_data(ini_string, wk_dir)

    # Write the updated ini file
    args, inputs = write_ini_file(args)

    # Generate the submission scripts
    slurm_script = write_submission_scripts(inputs)

    # If the job is open, we need to run the data generation step on the head nodes (ozstar specific) because compute
    # nodes do not have internet access.
    if not args.gaussian_noise or args.n_simulation == 0:
        # Process the slurm scripts to remove the data generation step
        data_gen_command = refactor_slurm_data_generation_step(slurm_script)

        # Run the data generation step now
        run_data_generation(data_gen_command, wk_dir)

    # Actually submit the job
    sched = get_scheduler()
    submit_bash_id = sched.submit(slurm_script, wk_dir)

    # If the job was not submitted, simply return. When the job controller does a status update, we'll detect that
    # the job doesn't exist and report an error
    if not submit_bash_id:
        return None

    # Create a new job to store details
    job = {
        'job_id': get_unique_job_id(),
        'submit_id': submit_bash_id,
        'working_directory': wk_dir,
        'submit_directory': inputs.submit_directory
    }

    # Save the job in the database
    update_job(job)

    # return the job id
    return job['job_id']
